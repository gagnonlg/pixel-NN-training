# Documentation for pixel-NN-training
# louis.guillaume.gagnon@cern.ch

Here is an example to fully train an test a neural network.

The trainNN_keras.py script uses the keras framework to train a
neural network.

You will need to have keras installed on your machine. Refer to the
installation instruction at http://keras.io

Here are the options to the script:
$ python2 trainNN_keras.py --help
usage: trainNN_keras.py [-h] --training-input TRAINING_INPUT --output OUTPUT
                        --config CONFIG [--shape SHAPE SHAPE]
                        [--validation-fraction VALIDATION_FRACTION]
                        [--structure STRUCTURE [STRUCTURE ...]]
                        [--activation {sigmoid,tanh,relu}]
                        [--output-activation {softmax,linear}] [--l2 L2]
                        [--learning-rate LEARNING_RATE] [--momentum MOMENTUM]
                        [--batch BATCH] [--min-epochs MIN_EPOCHS]
                        [--max-epochs MAX_EPOCHS]
                        [--patience-increase PATIENCE_INCREASE]
                        [--threshold THRESHOLD] [--no-normalize] [--verbose]

Refer to the "parse_args" function at the top of the script for the
default values. The config files is used to tell the scripts which
variables are the inputs and which are the targets. It's also used to
specify which variable are metadata to be saved by the evalNN script -
this block is ignored when training. See the example number.config.txt.

$ setupATLAS
$ lsetup root
$ python2 trainNN_keras.py --training-input $INPUT --output test  \
    --config number.config.txt

This should create the following files:
test.model.yaml
test.normalization.txt
test.weights.hdf5

You can then use them to test the neural network. The workflow I
prefer uses the "evalNN_keras.py" script to first evaluate the test
set and store the result in an sql database which can then be queried
to calculate different metrics.

$ python2 evalNN_keras.py --help
usage: evalNN_keras.py [-h] --input INPUT --model MODEL --weights WEIGHTS
                       --config CONFIG --output OUTPUT
                       [--normalization NORMALIZATION]

The --config file is the same as for the training script, but the
metadata block is saved in the ouput database.

$ python2 evalNN_keras.py --input $TEST_INPUT --model test.model.yaml \
    --weights test.weights.hdf5 --normalization test.normalization.txt \
    --output test.db --config number.config.txt

This should create an sqlite3 database at "test.db".

If you are evaluating the number network, you would then produce ROC
curves. See the "test-driver.sh" script for an example of how I do
it. In short, it uses a python script to generate all the sql queries
to extract subsets of clusters and feeds them to the ROC.cxx and
ROC_Graph.cxx programs in parallel.

##### TODO ####################

* Add evaluation code for AGILEPack
* Add config files for "old" format
* Generate static lists of SQL queries for number and position neural network ROCS
* Write a more generic version of test-driver.sh
* Port the position NN test code which produces residual distributions
* Add script to randomly generate NN hyperparameters configurations
* Add driver script for optimization