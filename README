# Documentation for pixel-NN-training
# louis.guillaume.gagnon@cern.ch

Here is an example to fully train an test a neural network.

The trainNN_keras.py script uses the keras framework to train a
neural network.

You will need to have keras installed on your machine. Refer to the
installation instruction at http://keras.io

Here are the options to the script:
$ python2 trainNN_keras.py --help
usage: trainNN_keras.py [-h] --training-input TRAINING_INPUT --output OUTPUT
                        [--shape SHAPE SHAPE]
                        [--validation-fraction VALIDATION_FRACTION]
                        [--targets TARGETS]
                        [--structure STRUCTURE [STRUCTURE ...]]
                        [--activation {sigmoid,tanh,relu}]
                        [--output-activation {softmax,linear}] [--l2 L2]
                        [--learning-rate LEARNING_RATE] [--momentum MOMENTUM]
                        [--batch BATCH] [--min-epochs MIN_EPOCHS]
                        [--max-epochs MAX_EPOCHS]
                        [--patience-increase PATIENCE_INCREASE]
                        [--threshold THRESHOLD] [--verbose]

optional arguments:
  -h, --help            show this help message and exit
  --training-input TRAINING_INPUT
  --output OUTPUT
  --shape SHAPE SHAPE
  --validation-fraction VALIDATION_FRACTION
  --targets TARGETS
  --structure STRUCTURE [STRUCTURE ...]
  --activation {sigmoid,tanh,relu}
  --output-activation {softmax,linear}
  --l2 L2
  --learning-rate LEARNING_RATE
  --momentum MOMENTUM
  --batch BATCH
  --min-epochs MIN_EPOCHS
  --max-epochs MAX_EPOCHS
  --patience-increase PATIENCE_INCREASE
  --threshold THRESHOLD
  --verbose

Refer to the "parse_args" function at the top of the script for the
default values.

$ setupATLAS
$ lsetup root
$ python2 trainNN_keras.py --training-input $INPUT --output test

This should create the following files:
test.model.yaml
test.normalization.txt
test.weights.hdf5

You can then use them to test the neural network. The workflow I
prefer uses the "evalNN_keras.py" script to first evaluate the test
set and store the result in an sql database which can then be queried
to calculate different metrics.

$ python2 evalNN_keras.py --help
usage: evalNN_keras.py [-h] --input INPUT --model MODEL --weights WEIGHTS
                       --config CONFIG --output OUTPUT
                       [--normalization NORMALIZATION]

optional arguments:
  -h, --help            show this help message and exit
  --input INPUT
  --model MODEL
  --weights WEIGHTS
  --config CONFIG
  --output OUTPUT
  --normalization NORMALIZATION

The --config file is used to tell the script which variables are
inputs for the NN, which are the targets, and which are metadata to
store in the db alongside the targets and predictions. You can find
an example at "number.test-config.txt"

$ python2 evalNN_keras.py --input $TEST_INPUT --model test.model.yaml \
    --weights test.weights.hdf5 --normalization test.normalization.txt \
    --output test.db --config number.test-config.txt

This should create an sqlite3 database at "test.db".

If you are evaluating the number network, you would then produce ROC
curves. See the "test-driver.sh" script for an example of how I do
it. In short, it uses a python script to generate all the sql queries
to extract subsets of clusters and feeds them to the ROC.cxx and
ROC_Graph.cxx programs in parallel.

##### TODO ####################

* Modify trainNN_keras.py to use the same --config scheme as evalNN
* Generate static lists of SQL queries for number and position neural network ROCS
* Write a more generic version of test-driver.sh
* Port the position NN test code which produces residual distributions
* Add script to randomly generate NN hyperparameters configurations
* Add driver script for optimization